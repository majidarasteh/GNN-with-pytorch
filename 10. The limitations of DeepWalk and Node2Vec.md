# The limitations of DeepWalk and Node2Vec

The **limitations of DeepWalk and Node2Vec**—computational inefficiency, lack of feature utilization, and non-inductive nature—motivate the need for more advanced techniques like graph **convolutional networks (GCNs)** such as **GraphSAGE** and **GAT** (Graph Attention Network) or other neural message-passing approaches such as **Message Passing Neural Network (MPNN)**. The limitations of DeepWalk and Node2Vec are as follow:  

1. **Parameter Sharing & Computational Efficiency:** DeepWalk/Node2Vec rely solely on graph structure (edges), ignoring node attributes (e.g., text, images, or metadata). GCNs naturally integrate node features by propagating them through the graph (e.g., via feature aggregation from neighbors). For example in a citation network, a GCN can use both citation links and paper text to generate embeddings.
   
2. **Incorporating Node Features (Semantic Information):** DeepWalk/Node2Vec rely solely on graph structure (edges), ignoring node attributes (e.g., text, images, or metadata). GCNs naturally integrate node features by propagating them through the graph (e.g., via feature aggregation from neighbors). For example in a citation network, a GCN can use both citation links and paper text to generate embeddings.
   
3. **Inductive Learning (Handling Unseen Nodes:)** DeepWalk/Node2Vec are transductive—they require retraining for new nodes. GCNs are inductive—they learn a generalizable function (e.g., a neural network) that can embed unseen nodes if their features and local structure are provided. For example a GCN trained on a social network can embed a new user without retraining, as long as their friends and profile features are known.

## Challenges in applying convolutional methods to graph-structured data  
The critical challenges in applying convolutional methods to graph-structured data (unlike grid-like images) are as follow: 
1. **Variable Number of Nodes:** Unlike images (fixed grid size), graphs can have arbitrary numbers of nodes.
2. **Variable Node Distances (No Fixed Spatial Locality):** In images, convolution relies on fixed-distance neighbors (e.g., 3x3 kernels). Graphs have irregular connectivity.
3. **Variable Number of Features per Node:** Nodes may have different feature dimensions (e.g., users with varying metadata).
4. **Heterogeneous Graphs (Nodes/Edges of Different Types):** Nodes/edges may have different types (e.g., users, products, reviews).

   ## Simple Graph Convolution
   Simple Graph Convolution (SGC) is a minimal form of graph convolution where:
   
    1. **Aggregation:** Each node collects features from its neighbors (e.g., by averaging them). For a node $v$, aggregate features from its neighbors $N(v)$:

       ![image](https://github.com/user-attachments/assets/1dc827e1-c2ef-4404-aff6-ab82f318d77a)
    
          $h_u$ : Feature vector of neighbor $u$.

          **Example:** If node $B$ has neighbors $A$ and $C$, its aggregated feature is:

       ![image](https://github.com/user-attachments/assets/308197bc-b323-4a82-a9d9-f720245ea666)

      
    2. **Update:** Replace the node’s feature with the aggregated value:

       ![image](https://github.com/user-attachments/assets/0eaaa665-07a8-4eea-b698-690680990354)

        * **No learnable parameters:** Just neighbor averaging.  
        * **With parameters:** Add a weight matrix $W$ and activation (e.g., ReLU):

        ![image](https://github.com/user-attachments/assets/ba1037d3-1df7-4542-b134-76a868daf4ac)

    **Why Is This Simple?**
   
     1. **No feature transformation:** Raw neighbor features are averaged.
     2. **No self-loops:** Node’s own features are ignored unless explicitly added.
     3. **Single-hop:** Only immediate neighbors are considered (no multi-layer propagation).
  
## Example

![image](https://github.com/user-attachments/assets/4bc5c2ef-fce4-4bfd-b199-f33f6b8a0ee1)

![image](https://github.com/user-attachments/assets/31c270c6-667c-48f7-8296-dbc882f41ce5)  



