# How to learn the parameters ($W$ and $a$)?
To learn the parameters ($W$ and $a$) in Graph Attention Networks (GAT) or GraphSAGE, we use loss function to minimize a loss function. Here’s a clear breakdown of the three approaches to minimize the loss function.
1. **Supervised Learning:** In this approach we have node features $X$ and graph structure (edges) and the goal is to minimize loss using labeled data.  
   * **forward pass:** At first, we compute $h_i$, $e_{ij}$, $α_{ij}$, and $z_i$. Then labels will be predicted label through a classifier (e.g., softmax).
   * **Calculate Loss:** Compare predictions (y_pred) to ground-truth labels (y_true). This measures how wrong the predictions are.  
     loss = F.cross_entropy(model_output, true_labels)

     ![image](https://github.com/user-attachments/assets/0f97b4d1-99fa-4256-8844-06d3852a08da)

   * **Backward Pass (Gradient Calculation):** Use backpropagation to compute gradients of the loss $W$ and $a$. Gradient for weight matrix and attention vector are computed as follow. PyTorch and TensorFlow do this automatically with loss.backward().  
    $∇W = ∂loss/∂W$   
    $∇a = ∂loss/∂a$

   * **Update Parameters:** Adjust $W$ and $a$ to minimize the loss using an optimizer (e.g., Adam):  
     $W = W - learningrate * ∇W$  
     $a = a - learningrate * ∇a$  
     
1. **Unsupervised Learning (No Labels) in GraphSAGE:** In this approach connected nodes should have similar embeddings and random (unconnected) nodes should have dissimilar embeddings. This approach uses a loss function based on node similarity (dot product + sigmoid). The loss function is designed to learn node embeddings by:  
   * Pulling together embeddings of connected nodes (positive pairs).  
   * Pushing apart embeddings of random node pairs (negative samples).
     
   Equation:  
   ![image](https://github.com/user-attachments/assets/5144a0d9-a4e2-48dd-b8cc-fe9def4308fa)

   * $z_u$, $z_v$: Embeddings of connected nodes
   * $z_{v_n}$: Embedding of a negative sample (unconnected node).
   * $σ$: Sigmoid function.
   * $Q$: Number of negative samples per positive pair. Controls how aggressively penalize dissimilarity (regularization parameter).
   * $P_n(v)$: Negative sampling distribution (e.g., uniform or degree-based).

   **How It Works: Step-by-Step**   
   1. Positive Pairs (Connected Nodes): Goal is to maximize similarity for connected nodes ($u,v_n$). Mechanism is as follow:
      * Compute dot product $z_u z_v$. If $z_u$ and $z_v$ are similar, the dot product is a **large positive number**.
      * Pass through sigmoid: σ(large positive)≈1.
      * Take negative log: −log(1)≈0 (minimized loss).
        
   2. Negative Pairs (Unconnected Nodes): Goal is to minimize similarity for random node pairs ($u,v$). Mechanism is as follow:  
      * Compute dot product $z_u z_{v_n}$. If $z_u$ and $z_{v_n}$ are dissimilar, the dot product is a **large negative number**.  
      * Negate and pass through sigmoid: σ(−large negative)=σ(large positive)≈1.
      * Take negative log: −log(1)≈0 (minimized loss).

   **Explain**  
   * Dot Product measures similarity in embedding space.
   * sigmoid maps similarity scores to probabilities (range [0,1]).
   * Logarithm heavily penalizes incorrect predictions (e.g., σ($z_u^⊤,z_v$)≈0 for positive pairs).
  
    The unsupervised loss function introduced in GraphSAGE is a **general-purpose objective** that can be applied to any graph neural network (GNN) to learn node embeddings without labeled data. This function does not require node/edge labels and only requires the GNN to produce embeddings $z_u$ ($z_u$ represents the embedding or latent representation of node $u$ after being processed by the GNN). It can be applied to the both homogeneous graphs (e.g., social networks) and heterogeneous graphs (e.g., knowledge graphs) with minor adjustments..
     
2. **Semi supervised:** In semi-supervised learning with Graph Neural Networks (GNNs), we combine labeled and unlabeled data to improve model performance, especially when labels are scarce. The Semi-Supervised Loss Formula with Regularization ($λ$) is as follow:

   ![image](https://github.com/user-attachments/assets/41a5954f-f7c7-45c5-9a4c-e11ac358d3cf)

   The **regularization ($λ$)** balances contribution of supervised vs. unsupervised losses and it's typical range: $λ∈[0.1,1.0]$.
   * If $λ=0$, then pure supervised learning (ignores graph structure).
   * If $λ → 1$, then strong emphasis on preserving connections.

   ![image](https://github.com/user-attachments/assets/81554ae2-ab58-4280-b31a-5d9a08c7c9b4)
