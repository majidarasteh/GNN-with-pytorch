# Simple Graph Convolution

## The limitations of DeepWalk and Node2Vec

The **limitations of DeepWalk and Node2Vec**—computational inefficiency, lack of feature utilization, and non-inductive nature—motivate the need for more advanced techniques like graph **convolutional networks (GCNs)** such as **GraphSAGE** and **GAT** (Graph Attention Network) or other neural message-passing approaches such as **Message Passing Neural Network (MPNN)**. The limitations of DeepWalk and Node2Vec are as follow:  

1. **Parameter Sharing & Computational Efficiency:** DeepWalk/Node2Vec rely solely on graph structure (edges), ignoring node attributes (e.g., text, images, or metadata). GCNs naturally integrate node features by propagating them through the graph (e.g., via feature aggregation from neighbors). For example in a citation network, a GCN can use both citation links and paper text to generate embeddings.
   
2. **Incorporating Node Features (Semantic Information):** DeepWalk/Node2Vec rely solely on graph structure (edges), ignoring node attributes (e.g., text, images, or metadata). GCNs naturally integrate node features by propagating them through the graph (e.g., via feature aggregation from neighbors). For example in a citation network, a GCN can use both citation links and paper text to generate embeddings.
   
3. **Inductive Learning (Handling Unseen Nodes:)** DeepWalk/Node2Vec are transductive—they require retraining for new nodes. GCNs are inductive—they learn a generalizable function (e.g., a neural network) that can embed unseen nodes if their features and local structure are provided. For example a GCN trained on a social network can embed a new user without retraining, as long as their friends and profile features are known.

## Challenges in applying convolutional methods to graph-structured data  
The critical challenges in applying convolutional methods to graph-structured data (unlike grid-like images) are as follow: 
1. **Variable Number of Nodes:** Unlike images (fixed grid size), graphs can have arbitrary numbers of nodes.
2. **Variable Node Distances (No Fixed Spatial Locality):** In images, convolution relies on fixed-distance neighbors (e.g., 3x3 kernels). Graphs have irregular connectivity.
3. **Variable Number of Features per Node:** Nodes may have different feature dimensions (e.g., users with varying metadata).
4. **Heterogeneous Graphs (Nodes/Edges of Different Types):** Nodes/edges may have different types (e.g., users, products, reviews).

   ## Simple Graph Convolution concept

   Simple Graph Convolution (SGC) is a streamlined version of graph neural networks that reduces complexity while maintaining effectiveness. SGC simplifies graph convolution by removing nonlinear activations between layers, collapsing multiple propagation steps into a single operation, and separating feature smoothing from transformation. Here's mathematical formulation: for a graph with:
   * $A$: Adjacency matrix (with self-loops). Is a $N×N$ matrix where $N$ is the number of nodes.
   * $D$: Degree matrix. Is a $N×N$ matrix
   * $X$: Node features. Is a $N×f$ matrix, where $f$ is the number of features that each node has.
   * $K$: Propagation steps
   * $I$: Identity matrix (same size as $A$). The identity matrix is added to the adjacency matrix $A$ to ensures each node retains its own features during propagation (Include self-loops). And also, prevent feature degradation. Without $I$, a node's features would only come from neighbors, losing its original information.
     
    SGC computes:  
    ![image](https://github.com/user-attachments/assets/701d204c-345c-48a2-b776-19770120dab2)

   **Key Components:**

    * Normalized Adjacency Matrix: Symmetric normalization preserving scale
      
      ![image](https://github.com/user-attachments/assets/0f2e7b20-5cb4-4c70-8bd6-d5d708f31fc6)
      
    * Feature Propagation: For example, $K=1:$ is the immediate neighbors and $K=2$ is the neighbors' neighbors. Result of the following formula is $N×f$ matrix.

      ![image](https://github.com/user-attachments/assets/51605894-89e3-4143-bf8c-6009e235aa2a)
      
    * Linear Classification. Computed according the following equation:

      ![image](https://github.com/user-attachments/assets/f603644d-e591-4c52-820d-18cf9a8a13f9)
     
        Simple Graph Convolution (SGC) does not use activation functions (like ReLU, sigmoid, etc.). This is a key distinction from traditional Graph Convolutional Networks (GCNs).

       ![image](https://github.com/user-attachments/assets/a1449983-5dbe-4f5d-b23d-d4ac51edc4e5)

  
## Example

![image](https://github.com/user-attachments/assets/d2d390ba-63c9-4c05-91b3-7d5ec89f67c8)

![image](https://github.com/user-attachments/assets/0d1659d0-0b70-4df5-93a1-84e76a83b2d7)

![image](https://github.com/user-attachments/assets/0a711cd2-4632-4dd7-92e5-c902f9482fbc)

 
## Similarity in Embedding Space  

The goal is to measure how "similar" two nodes are based on their embeddings $V(K)$. Common similarity metrics:  

 1) **Cosine Similarity:**  Values in $[−1,1]$. Used for node similarity (e.g., recommendation systems).

     ![image](https://github.com/user-attachments/assets/65cb93d5-aa0c-4514-92de-5cc2f91ff216)

 3) **Euclidean Distance:**  Smaller distance → higher similarity.

    ![image](https://github.com/user-attachments/assets/2dbb1102-a133-4e95-8fa9-6824828f10de)

## Drawbacks of Simple Graph Convolution (SGC)  
Simple Graph Convolution (SGC) is a foundational method for graph-structured data, but it has several key limitations that hinder its performance in complex scenarios. Here are the main drawbacks:  

1) **Equal Treatment of Neighbors (No Attention Mechanism):** SGC averages all neighboring nodes equally, ignoring differences in their importance. For example in a social network, a celebrity’s post should influence a node more than a random user’s post, but SGC treats them the same. For handling this problem, Graph Attention Networks (GAT) assign learnable weights to neighbors.
2) **Fixed Averaging Aggregation (May Not Be Optimal):**  Mean aggregation assumes all neighbors contribute equally, which isn’t always true. For example In fraud detection, some transactions (edges) are more suspicious than others. For instance, GraphSAGE uses max-pooling or LSTM aggregation.
3) **SNo Node Feature Transformation (Limited Expressiveness):** SGC simply averages raw features without learning feature transformations. For example if two neighbors have opposite features (e.g., $[1,0]$ and $[-1,0]$), their average ([0,0]) loses information. For example for handling this problem GCN adds a learnable weight matrix:  

     ![image](https://github.com/user-attachments/assets/c928ee25-43f5-4227-af5a-ee474f39cd6b)  

5) **Over-Smoothing (Deep SGC Blurs Node Features):** After many layers, node embeddings become indistinguishable because repeated averaging diffuses features globally. For example, in a 10-layer SGC, nodes in a community may all converge to the same embedding.

