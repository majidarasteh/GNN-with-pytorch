# From DeepWalk to Node2Vec  

**Node2Vec** is an advanced graph embedding algorithm that extends DeepWalk by **introducing biased random walks** to better capture both local neighborhood structures and global community patterns in networks. Unlike **DeepWalk's purely random (unbiased) walks**, Node2Vec employs two tunable parameters: **return parameter (p)** and **in-out parameter (q)**, to interpolate between breadth-first search (BFS) and depth-first search (DFS) behaviors. This allows it to flexibly emphasize either homophily (nodes in the same community) or structural equivalence (nodes with similar roles, e.g., hubs or bridges). By balancing these exploration strategies, Node2Vec generates embeddings that preserve richer network properties, making it particularly effective for tasks like node classification, link prediction, and anomaly detection in complex real-world graphs such as social networks or biological interactions. Its efficiency and interpretable parameters have made it a widely adopted tool in graph machine learning.  

In graph traversal, **Breadth-First Search (BFS)** and **Depth-First Search (DFS)** represent two fundamentally different exploration strategies, each offering unique advantages for learning graph representations:  
 - **BFS (Local Exploration):** Explores a node’s immediate neighborhood before moving further away. BFS prioritizes revisiting nearby nodes, making it ideal for capturing local structural patterns (e.g., community membership, direct relationships). For example, in a social network, BFS would emphasize a user’s close friends, making their embeddings reflect tight-knit groups. BFS preserves micro-scale features (nodes with similar local roles, like bridges or hubs). BFS is like exploring a city block-by-block, learning each neighborhood’s unique traits.
 - **DFS (Global Exploration):** Moves as far as possible from the starting node before backtracking. DFS Discovers distant nodes and long-range dependencies, useful for identifying broader community structures or hierarchical roles. For example, in a citation network, DFS might traverse multiple research fields, embedding papers by their thematic connections rather than direct citations. DFS captures macro-scale features (nodes in the same community or distant but related clusters). DFS is like taking a cross-country road trip, understanding how regions connect at a larger scale.

**Node2Vec** bridges these strategies by tuning $p$ (BFS-like behavior) and $q$ (DFS-like behavior), enabling a controllable balance between local and global perspectives. By combining both, Node2Vec generates richer embeddings than pure random walks (DeepWalk), adapting to diverse graph topologies and tasks. Following tabel compares random Walk strategies: DeepWalk vs Node2Vec.  

![image](https://github.com/user-attachments/assets/bf397f31-3049-42c9-8d98-5d20ae126b42)   

## Mathematical Formulation  

Given a current node $v$, the probability of transitioning to its neighbor $u$ is defined as:  

![image](https://github.com/user-attachments/assets/9089432f-59f3-4acd-a081-98cdf7ec6767)  

**Interpretation**  

 1.  If $p$ is small, the walk is less likely to return to $t$ (encourages exploration).
 2.  If $q$ is small, the walk prefers outward nodes (DFS-like).
 3.  If $q$ is large, the walk stays local (BFS-like).
  
**In another words**  

1. When $p=q=1$, Node2Vec reduces to DeepWalk.
2. For community detection, use $p>1$, $q<1$ (DFS emphasis)
3. For role discovery, use $p<1$, $q>1$ (BFS emphasis)

This formulation enables Node2Vec to interpolate between BFS and DFS behaviors, making it more flexible than DeepWalk for complex networks. 

## Positive and Negative Walk  

In Node2Vec (and similar random-walk-based embedding methods), positive and negative walk samples are key components of the training process. They are used to train the model to distinguish between nodes that appear together in walks (positive pairs) and random node pairs that do not (negative pairs). Here's a detailed breakdown:  

1. **Positive Walk Samples:** Subsequences of nodes that co-occur within a context window in a random walk. For example, for a walk $[A → B → C → D]$ with context_size=2, $(A, B), (A, C), (B, C), (B, D), (C, D)$ are positive pairs. Purpose is teach the model to assign similar embeddings to nodes that appear close in walks. Positive Loss maximizes similarity for positive pairs.
   
2. **Negative Walk Samples:** Randomly sampled node pairs that do not appear together in walks. Represent non-relationships (e.g., two nodes rarely/never connected). For exapmle, for the same walk $[A → B → C → D]$, negative pairs might be $(A, X), (B, Y), (C, Z)$, where $X, Y, Z$ are randomly chosen nodes not in the walk. Negative Loss minimizes similarity for negative pairs.
