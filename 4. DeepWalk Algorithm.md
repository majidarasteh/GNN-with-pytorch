## DeepWalk Algorithm: A Foundational Graph Embedding Method
DeepWalk is a pioneering **unsupervised graph** embedding technique that learns low-dimensional vector representations of nodes by leveraging **random walks** and **Skip-gram** (from NLP). It is particularly effective for preserving community structure in graphs.  

In DeepWalk, the similarity between two nodes $u$ and $v$ in the original graph space is implicitly defined by the probability of reaching $u$ via a random walk starting at $v$. This captures higher-order neighborhood relationships beyond immediate adjacency.  

The similarity between two nodes $u$ and $v$ in the original graph space is fundamentally defined as:  
![image](https://github.com/user-attachments/assets/8658742e-043d-46f6-9013-96f70d5997bb)  
This measures how likely a random walker starting at $v$ will reach $u$ within a given number of steps. At each step, the walker moves uniformly to a neighboring node. For a walk of length $T$ the probability is computed over all possible paths from $v$ to $u$ within $T$ steps. For example, user $v$ starts a random walk and user $u$ appears in 60% of walks (e.g. length $T=10$) from $v$. So, $similarity(u, v)=0.6$.  

**Practical Implications**
 - Community Detection: High-probability node pairs cluster together in embedding space.
 - Anomaly Detection: Low-probability pairs (unexpected connections) stand out.
 - Link Prediction: Missing edges are predicted between high-similarity nodes.

## Random walk  


**A) Key Components:**  

 1. **Random Walk Length ($T$):** $T$ is a hyperparameter controlling how far the walk explores from the starting node.  
    - Short walk (small $T$): is a hyperparameter controlling how far the walk explores from the starting node.
    - Long walks (larg $T$): Encode global, multi-hop relationships (macroscopic structure).
 2. **Similarity Definition:** Computed empirically by simulating many random walks.   
    - $sim(u,v):$ Probability of visiting $u$ in a $T-step$ walk starting at $v$. For example, if $T=3$, measures the likelihood of reaching $u$ within 3 steps from $v$.

**B) Role of $T$ in DeepWalk**   
Controls the trade-off between local and global similarity.  

![image](https://github.com/user-attachments/assets/5206a625-2ff6-4086-979b-bbd0be9831ea)  

**C) How $T$ affects embeddings**  
 1. Short $T$: Embeddings prioritize direct connections (like friend-of-friend in social networks).
 2. Long $T$: Embeddings reflect broader roles (e.g., nodes in the same community even if not directly linked).

**D) Choosing $T$ in Practice**  
 1. Default: Start with $T=40$ (common in literature).
 2. Link prediction: Prefer shorter $T$ (2-10).
 3. Community detection: Use longer $T$ (20-80).
    
For example, For a social network with dense communities: set $T=30$ to capture both close friendships (local) and group affiliations (global).

# Calculating Random Walk Probabilities: A Step-by-Step Explanation  

To compute the probability $S_{original}(u,v)$ (the likelihood of reaching node $u$ in a $T-step$ random walk starting at $v$), we follow these steps:  
 1. **Empirical Estimation via Repeated Random Walks**
    - Process:
      * Start at node $v$.
      * For each step $t≤T$, move uniformly to a random neighbor ($T:$ Walk Length).
      * Record whether $u$ is visited during the walk ($N:$ Number of Walks).
      * Repeat this process $N$ times (e.g., $N=1000$).
    - Probability Calculation  
      * ![image](https://github.com/user-attachments/assets/b8873985-9935-4cfb-ac0f-faf3d65a7e4c)
       
 
 2. **Mathematical Formulation (Transition Matrices)**
    
    For larger graphs, we can compute probabilities analytically using the graph’s transition matrix $P$:  
    - Transition Matrix $P$:  Each entry $P_{ij}$ is the probability of moving from node $i$ to $j$.  
      ![image](https://github.com/user-attachments/assets/21bba8a6-4802-4cc6-ae72-05261938315c)

    - T-Step Probability:
      The probability of reaching $u$ from $v$ in exactly $T$ steps is:   
      ![image](https://github.com/user-attachments/assets/6f9a2975-e7c3-4c26-a2da-3150fb20dd72)
      

   3. **Example Calculation**
      
      ![image](https://github.com/user-attachments/assets/82da714f-f8fb-478a-a41e-57eef5b32a07)
      
      In summary, the probability of reaching node $u$ in a random walk starting from $v$ directly determines their similarity in the original graph space. Here’s a breakdown of why this works and what it implies:

      - **High Probability $(P(u∣v)≈1)$ → High Similarity:** If $u$ is almost always reachable from $v$ within $T$ steps. It means, they are directly connected or part of the same dense community.
      - **High Probability $(P(u∣v)≈0)$ → Low Similarity:** If $u$ is rarely/never reached from $v$. It means, they are separated by many hops or in disconnected components.
      - **For example:**  
        ![image](https://github.com/user-attachments/assets/7c464f2e-2de5-42dc-b517-6041e2e24467)


## Similarity in the embedding space  

1. **Embedding Space Similarity ($S_E$)**
   
   For nodes $u$ and $v$ with embeddings $z_u$ and $z_v$:  
   ![image](https://github.com/user-attachments/assets/42301d85-7de6-4f40-84ba-f4b1e346f530)  

   **Interception:**
    - Large positive dot product → High similarity (vectors point in similar directions).
    - Near zero → Orthogonal (no relationship).
    - Negative → Dissimilar (opposite directions).
      
 2. **Converting to Probability with Softmax**
    To interpret $S_E(u,v)$ as a probability (matching the random walk co-occurrence likelihood), DeepWalk uses the softmax over all nodes:  
    
    ![image](https://github.com/user-attachments/assets/959dbfbc-e31e-4a6f-ae6b-3bd1d07fddbd)  
       
 3. **Practical Implications**

    ![image](https://github.com/user-attachments/assets/e64fac8e-5d50-4ba1-b0ee-a65a3b5aba91)  

 4. **Example**

    ![image](https://github.com/user-attachments/assets/94c4f942-0cb4-4abd-b3be-0b6689ce3d91)

## DeepWalk's Lookup Table Encoder  

DeepWalk uses a lookup table (embedding matrix) where each node is represented by a trainable D-dimensional vector. This is the simplest form of graph encoder. Just a direct mapping from node IDs to vectors.  

 1. **The Lookup Table Structure**
    - Each row corresponds to a node's embedding.
    - D is a hyperparameter you choose (typically 64-256).
      
      ![image](https://github.com/user-attachments/assets/70756139-1284-4f69-b902-9580f66cfb4e)

    **Key Properties**
    - Trainable: The vectors are optimized during training.
    - No Input Features: Only node IDs are used (unlike GNNs that use node attributes).
    - Scalability: Efficient for large graphs (memory grows linearly with num_nodes × D).  
       
 2. **How It Captures Graph Relationships**
    - Connected nodes (or nodes co-occurring in random walks) have similar embeddings (high dot product).
    - Unrelated nodes have dissimilar embeddings (low dot product).
      
 3. **Defining the Embedding Dimension (D)**
    - Small D (e.g., 2-10):  
      * Pros: Easy to visualize (2D/3D plots), faster training.
      * Cons: May lose weak relationships.
    - Large D (e.g., 128-256):  
      * Pros: Captures complex patterns, better for downstream tasks.
      * Cons: More memory/compute required.
        
 4. **DeepWalk's Optimization Process: Minimizing the Loss Function**

    DeepWalk frames node embedding as an optimization problem, where we define a loss function based on similarities $S_E$ (embedding space) and $S_G$ (graph space). Then, iteratively adjust the lookup table values to minimize this loss. Lets crystallize this process:
    
    - Graph Space ({S_G}): Random walk co-occurrence probability.

      ![image](https://github.com/user-attachments/assets/09f32c13-fcb1-4be8-b7b9-60d460268064)
  
    - Embedding Space ({S_E}): Dot product of learned vectors.  

      ![image](https://github.com/user-attachments/assets/cd1234f5-65aa-487b-898c-4d001ae4a74f)

    - Loss function:  
      ![image](https://github.com/user-attachments/assets/3402e16d-69c9-47c9-8d1c-acc731f7524f)  
   
## What is Skip-gram?
Skip-gram is a neural network model that learns meaningful vector representations (embeddings) of words (or nodes in graphs) by predicting context given a target. It’s like teaching the model to guess "which words/nodes usually appear around this word/node?" Skip-gram is a powerful tool to convert discrete words/nodes into vectors that preserve their contextual relationships.  

1. **How Skip-gram Works**
   - Example 1: Words (Original NLP Version)
     * Sentence: "The quick brown fox jumps"
       - Target word: "brown"
       - Context window (size=1): ["quick", "fox"]

      The model learns:
      - When you see "brown", predict nearby words ("quick", "fox")
      - Result: "fox" and "brown" get similar vector representations  
    
   - Example 2: Graph Nodes (DeepWalk Adaptation)
     * Random walk: [Library → Student → Book → Computer]
       - Target node: "Book"
       - Context window (size=1): ["Student", "Computer"]

      The model learns:
      - "Book" should predict its walk neighbors ("Student", "Computer")
      - Result: "Book" and "Library" (which share similar contexts) get similar embeddings
        
2. **Why It's Powerful for Graphs**
   - Captures Proximity: Nodes appearing in similar walk contexts get similar vectors.
   - Scalable: Processes walks sequentially (no huge matrices).
   - Flexible: Works for any graph structure.
     
3. **Example**  
   ![image](https://github.com/user-attachments/assets/545073bc-8add-4f1b-99bd-31e91dee5673)
 








