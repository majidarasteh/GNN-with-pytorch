# Graph Attention Network (GAT) 
**Graph Attention Network (GAT)** was introduced by Petar Veličković et al. in 2017. GAT introduces attention mechanisms to graph neural networks, allowing nodes to dynamically weigh the importance of their neighbors during feature aggregation. Unlike GCNs (which treat all neighbors equally), GAT learns to focus on relevant connections. Graph Attention Networks (GATs) indeed lies in the attention weight $α_{ij}$, which dynamically quantifies the importance of node $j's$ features to node $i$. For example, in a social network, your close friends’ opinions might matter more than distant connections.

## Definition of $α_{ij}$ in GAT
The attention weight $α_{ij}$ is computed in three steps:  
1. **Linear Transformation**  
   In Graph Attention Networks (GAT), the node features $h_i$ and $h_j$ are numerical vectors that represent the characteristics or attributes of nodes $i$ and $j$ in a graph. These features are the input data for the GAT model and are used to compute attention weights ($α_{ij}$). Their shapes is $F$, where $F$ = feature dimension. For example, in a social network, features could include: user profile data such as age and interests, activity metrics such as enumber of posts and likes.  In a citation network (like Cora dataset) features could include: Bag-of-words representation of a paper’s text and Metadata such as publication year.

   ![image](https://github.com/user-attachments/assets/08199dc5-6388-4834-8372-5efdd83c8c93)

   In Graph Attention Networks (GAT), $Wh_i$ and $Wh_j$ are transformed node features, created by multiplying the original features ($h_i$ and $h_j$) with a learnable weight matrix W. This projects features into a higher-level space ($F′$ dimensions).  $Wh_i$ and $Wh_j$ Allow the model to learn which features are important for attention. Therefore:

   ![image](https://github.com/user-attachments/assets/0b658328-85c5-490c-9610-6922ca18dbc2)

2.  **Attention Score** $e_{ij}$:  
   In Graph Attention Networks (GAT), the vector $a$ (often called the "learnable attention vector") is a critical component that determines how much importance (attention) node $i$ should assign to its neighbor $j$. It acts like a "scorer" that evaluates the compatibility between two nodes. For example:  

    ![image](https://github.com/user-attachments/assets/dc172806-879e-4bbc-b2ec-b64ee20476a4)  

    The $a$ allows the model to focus on relevant neighbors dynamically (unlike GCN’s fixed weights) and Computes the unnormalized attention score $e_{ij}$ between node $i$ and its neighbor $j$.

    ![image](https://github.com/user-attachments/assets/4968fcd2-994f-4811-9d8c-53b7369b3754)

4. **Normalization with Softmax:**
   Scores are normalized over node $i$’s neighborhood $N_i$ using softmax:

      ![image](https://github.com/user-attachments/assets/34a84016-4d17-4593-bdba-427ce4fe4fb4)

    So we have:  
    ![image](https://github.com/user-attachments/assets/d39d7726-517f-47ac-8d0a-efcbfd2a6ff7)


5. **Key Formula:**  
   This defines how GAT dynamically prioritizes neighbors.

   ![image](https://github.com/user-attachments/assets/389b2be5-76ad-4fb8-ba42-e8631b07ac87)


## Step-by-Step Numerical Example  
Let's walk through a step-by-step numerical example of how a Graph Attention Network (GAT) computes attention weights ($α_{ij}$) between two nodes $i$ and $j$. We'll define all variables and show calculations with actual numbers.  
1. **Define Input Features:** Raw node features (e.g., 2-dimensional for simplicity)
   * $h_i=[1.0,2.0]$ (features of node $i$)
   * $h_j=[0.5,1.5]$ (features of node $j$)
   
2. **Define Learnable Parameters:** Projects raw features to a higher-level space (e.g., 2D → 2D).
   
   ![image](https://github.com/user-attachments/assets/6036d805-a1ce-4d46-9519-36e20a4eda13)

3. **Attention Vector ($a$):** Evaluates compatibility between nodes (length $2F′=4$ since we concatenate $Wh_i$ and $Wh_j$).  
   a=[0.1, −0.2, 0.3, 0.4]
   
4. **Linear Transformation ($Wh_i$ and $Wh_j$)** Transform features using W.
   
   ![image](https://github.com/user-attachments/assets/ffd77f38-2b28-475e-950f-6dfcad1b06ba)  

5. **Concatenate Transformed Features** Combine $Wh_i$ and $Wh_j$ for pairwise attention.
   
   $[Wh i∥Whj]=[0.5,1.1,0.35,0.95]$
   
6. **Compute Unnormalized Attention Score ($e_{ij}$):** Calculate the dot product with $a$ and apply LeakyReLU (slope=0.2).  

   ![image](https://github.com/user-attachments/assets/d7641b4a-cc84-4465-8747-bb314f7529bb)

7. **Normalize Attention Scores ($α_{ij}$):** Assume node $i$ has two neighbors ($j$ and $k$) with scores.

   ![image](https://github.com/user-attachments/assets/bbe59bd0-bcf9-4299-86b7-e0e56db38b2d)

   The $α_{ij}$ is learned from data, not fixed like in GCN, and $α_{ij}=0.55$ means node $j$ contributes 55% to $i$'s update. 

## Step by Step Update of $h'_i$
The new features $h'_i$ are a weighted combination of $j$ and $k'$s transformed features.  

1. **Given Data from Previous Steps** 
   
   ![image](https://github.com/user-attachments/assets/657b501c-cb04-49d0-8f6a-488913bf7b50)

   Neighbor $j$ ($α_{ij}$=0.55) contributed more to $h'_i$ than neighbor $k$. Comparison to GCN, GCN, Would average $Wh_j$ and $Wh_k$ equally (e.g., $0.5Wh_j + 0.5Wh_k$).

3. **Aggregate Neighbor Features** Multiply each neighbor's transformed features by their attention weight and sum:
   
   ![image](https://github.com/user-attachments/assets/d923232e-3350-40ec-be88-8ae0cfdb7d8c)

4. **Apply Nonlinearity ($σ$)** Assume $σ$ is the ELU activation (Exponential Linear Unit).

   ![image](https://github.com/user-attachments/assets/1ef260b6-1ce4-48c2-945f-8ff17d8e9b60)

5. sdf

  
  
