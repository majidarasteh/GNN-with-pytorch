# Graph Attention Network (GAT) 
**Graph Attention Network (GAT)** was introduced by Petar Veličković et al. in 2017. GAT introduces attention mechanisms to graph neural networks, allowing nodes to dynamically weigh the importance of their neighbors during feature aggregation. Unlike GCNs (which treat all neighbors equally), GAT learns to focus on relevant connections. Graph Attention Networks (GATs) indeed lies in the attention weight $α_{ij}$, which dynamically quantifies the importance of node $j's$ features to node $i$. For example, in a social network, your close friends’ opinions might matter more than distant connections.

## Definition of $α_{ij}$ in GAT
The attention weight $α_{ij}$ is computed in three steps:  
1. **Linear Transformation**  
   In Graph Attention Networks (GAT), the node features $h_i$ and $h_j$ are numerical vectors that represent the characteristics or attributes of nodes $i$ and $j$ in a graph. These features are the input data for the GAT model and are used to compute attention weights ($α_{ij}$). Their shapes is $F$, where $F$ = feature dimension. For example, in a social network, features could include: user profile data such as age and interests, activity metrics such as enumber of posts and likes.  In a citation network (like Cora dataset) features could include: Bag-of-words representation of a paper’s text and Metadata such as publication year.

   ![image](https://github.com/user-attachments/assets/08199dc5-6388-4834-8372-5efdd83c8c93)

   In Graph Attention Networks (GAT), $Wh_i$ and $Wh_j$ are transformed node features, created by multiplying the original features ($h_i$ and $h_j$) with a learnable weight matrix W. This projects features into a higher-level space ($F′$ dimensions).  $Wh_i$ and $Wh_j$ Allow the model to learn which features are important for attention. Therefore:

   ![image](https://github.com/user-attachments/assets/0b658328-85c5-490c-9610-6922ca18dbc2)


2.  **Attention Score** $e_{ij}$:

  
  
