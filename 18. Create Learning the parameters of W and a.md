# How to learn the parameters ($W$ and $a$)?
To learn the parameters ($W$ and $a$) in Graph Attention Networks (GAT) or GraphSAGE, we use loss function to minimize a loss function. Here’s a clear breakdown of the three approaches to minimize the loss function.
1. **Supervised Learning:** In this approach we have node features $X$ and graph structure (edges) and the goal is to minimize loss using labeled data.  
   * **forward pass:** At first, we compute $h_i$, $e_{ij}$, $α_{ij}$, and $z_i$. Then labels will be predicted label through a classifier (e.g., softmax).
   * **Calculate Loss:** Compare predictions (y_pred) to ground-truth labels (y_true). This measures how wrong the predictions are.  
     loss = F.cross_entropy(model_output, true_labels)
     
   * **Backward Pass (Gradient Calculation):** Use backpropagation to compute gradients of the loss $W$ and $a$. Gradient for weight matrix and attention vector are computed as follow. PyTorch and TensorFlow do this automatically with loss.backward(). 
    $∇W = ∂loss/∂W$   
    $∇a = ∂loss/∂a$

   * **Update Parameters:** Adjust $W$ and $a$ to minimize the loss using an optimizer (e.g., Adam):  
     $W = W - learningrate * ∇W$  
     $a = a - learningrate * ∇a$  
     
2. **Unsupervised Learning (No Labels) in GraphSAGE:** In this approach connected nodes should have similar embeddings and random (unconnected) nodes should have dissimilar embeddings. This approach uses a loss function based on node similarity (dot product + sigmoid). The loss function is designed to learn node embeddings by:  
   * Pulling together embeddings of connected nodes (positive pairs).  
   * Pushing apart embeddings of random node pairs (negative samples).
     
   Equation:  
   ![image](https://github.com/user-attachments/assets/5144a0d9-a4e2-48dd-b8cc-fe9def4308fa)

   * $z_u$, $z_v$: Embeddings of connected nodes
   * $z_{v_n}$: Embedding of a negative sample (unconnected node).
   * $σ$: Sigmoid function.
   * $Q$: Number of negative samples per positive pair. 
   * $P_n(v)$: Negative sampling distribution (e.g., uniform or degree-based).

   **How It Works: Step-by-Step**   
   1. Positive Pairs (Connected Nodes): Goal is to maximize similarity for connected nodes ($u,v_n$). Mechanism is as follow:
      * Compute dot product $z_u z_v$. If $z_u$ and $z_v$ are similar, the dot product is a **large positive number**.
      * Pass through sigmoid: σ(large positive)≈1.
      * Take negative log: −log(1)≈0 (minimized loss).
        
   2. Negative Pairs (Unconnected Nodes): Goal is to minimize similarity for random node pairs ($u,v$). Mechanism is as follow:  
      * Compute dot product $z_u z_{v_n}$. If $z_u$ and $z_{v_n}$ are dissimilar, the dot product is a **large negative number**.  
      * Negate and pass through sigmoid: σ(−large negative)=σ(large positive)≈1.
      * Take negative log: −log(1)≈0 (minimized loss).

   **Explain**  
   * Dot Product measures similarity in embedding space.
   * sigmoid maps similarity scores to probabilities (range [0,1]).
   * Logarithm heavily penalizes incorrect predictions (e.g., σ($z_u^⊤,z_v$)≈0 for positive pairs).
     
3. **Semi supervised** 
