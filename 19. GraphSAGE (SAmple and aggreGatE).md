# Evolution from Traditional Graph Algorithms to Modern Approaches
Graph representation learning has evolved significantly, shifting from hand-engineered features and matrix factorization to neural network-based inductive methods like GraphSAGE.  
1. **Limitations of Previous Methods (e.g., DeepWalk)** 
   * **No parameter sharing:** Each node's embedding is learned independently, which is inefficient.
   * **No semantic information:** These methods rely solely on graph structure (e.g., random walks) and ignore node features.
   * **Transductive nature:** They cannot generalize to unseen nodes or graphs without retraining.
   * **Approachs such as:**
     * **Random walks:** DeepWalk, Node2Vec (transductive, no parameter sharing).
     * **Matrix factorization:** Eigen decomposition, Laplacian-based methods (e.g., Spectral Clustering).
     * **Handcrafted features:** Degree, centrality, clustering coefficient.
       
2. **Advancements with Graph Convolutional Networks (GCNs) and Graph Neural Networks (GNNs)**
   * **Parameter sharing:** Convolution operations allow sharing parameters across nodes, improving efficiency.
   * **Semantic information:** By aggregating feature vectors from neighbors, these methods incorporate node features (e.g., text attributes, degrees) into the embeddings.
   * **Approachs such as:**
     * Graph Convolutional Networks (GCN), Simple Graph Convolution (SGC), and Graph Attention Network (GAT)
   * **Remaining Challenge: Inductive Learning:** Despite the improvements, GCNs and GNNs are still transductive in their original formulations. They require the entire graph during training and cannot generalize to unseen nodes or subgraphs without modification.
       
3. **GraphSAGE as a Solution**
   * **Inductive framework:** GraphSAGE addresses the limitations of earlier methods by introducing an inductive framework that leverages parameter sharing and semantic information (node features). It generalizes to unseen data by learning aggregation functions over local neighborhoods, making it suitable for dynamic or evolving graphs. This represents a significant step forward in graph representation learning, particularly for real-world applications involving unseen nodes or graphs.
   * GraphSAGE is suitable for production systems (e.g., recommendation engines, fraud detection) where graphs constantly evolve.

# Transductive vs. Inductive Embedding Methods in Graph Representation Learning  
1. **Transductive Learning** In transductive learning, node embeddings (e.g., $z_u$) are learned individually for each node in a fixed graph. The model requires all nodes to be present during training and cannot generalize to unseen nodes or new graphs without retraining. These methods such as DeepWalk, Node2Vec, GCN (original formulation) directly optimize embeddings for each node. Limitations:
   * **No generalization to new nodes:** If a new node is added, the model must be retrained.
   * **Computationally expensive:** Embeddings must be recomputed for the entire graph when new data arrives.
   * **Graph-dependent:** Embeddings are tied to a specific graph structure.
     
2. **Inductive Learning (GraphSAGE Approach)** Instead of learning individual embeddings, GraphSAGE learns a function that generates embeddings by sampling and aggregating features from a node’s local neighborhood. This allows the model to generalize to unseen nodes or entirely new graphs without retraining. Key Innovations:
   * **Parameterized Aggregation Functions:** Uses trainable aggregators (Mean, LSTM, Pooling) to combine neighborhood features.
   * **Neighborhood Sampling:** Efficiently samples fixed-size neighborhoods to handle large graphs.
   * **Supports Unseen Nodes:** At test time, embeddings for new nodes can be generated on-the-fly by applying the learned aggregation functions.
   * **Scalable:** Works on dynamic graphs (e.g., social networks, citation networks).
   * **Flexible:** Can be applied to graphs with or without node features.

3. **Comparison Summary**

   <img width="691" height="281" alt="image" src="https://github.com/user-attachments/assets/51cf50ea-2f34-421f-9ebf-73f6e3f6a397" />

# GraphSAGE: Inductive Representation Learning on Large Graphs

GraphSAGE (Sample And Aggregate) introduces an inductive framework for generating node embeddings by:
1. **Sampling** a fixed-size neighborhood around a target node.
2. **Aggregating** features from sampled neighbors (instead of relying on fixed embeddings).
3. **Updating** the target node’s representation using the aggregated neighborhood information.

Key Innovations of GraphSAGE is as follow:
1. **Inductive:** generalization to unseen nodes/graphs without retraining.
2. **Neighborhood Aggregation** For a target node:
   * **Step 1:** Sample neighbors (e.g., 1-hop or 2-hop).
   * **Step 2:** Aggregate their features (e.g., mean, max-pooling, LSTM).
   * **Step 3:** Update the target node’s embedding using the aggregated features.
3. **Hop Distance (Search Depth):** defines how far to explore (e.g., 1-hop = direct neighbors, 2-hop = neighbors of neighbors).
   * **Shallow hops:** Faster but miss higher-order structure.
   * **Deeper hops:** Capture global topology but increase computation.
     
4. **Neighborhood Sampling:**
   * Instead of using all neighbors, randomly sample a fixed number (e.g., 25 neighbors per hop).
   * Critical for scalability on large graphs (avoids memory overload).

## How GraphSAGE Works

1. The following figure illustrates the GraphSAGE sample approach.
   
   <img width="2844" height="1000" alt="image" src="https://github.com/user-attachments/assets/cd7ff983-f43e-4a6e-b45b-82b3171da424" />

2. GraphSAGE embedding generation algorithm:
   
   <img width="773" height="369" alt="image" src="https://github.com/user-attachments/assets/4bbb0d01-9e55-4f7b-b4fa-fbf533fa8269" />

3. Aggregator Functions:  
   * **Mean Aggregator:** Averages neighbor features (similar to GCN but inductive).
   * **LSTM Aggregator:** Uses LSTM on shuffled neighbors (expressive but not permutation-invariant).
   * **Pooling Aggregator:** Applies MLP + max-pooling (most powerful in practice).  

* **Mean Aggregator (Convolutional)**
  
  Takes the element-wise mean of neighbor features. For example:
  * Neighbors: $[0.3, 0.4], [0.5, 0.6], [0.7, 0.8]$
  * Aggregation: mean = $[(0.3+0.5+0.7)/3, (0.4+0.6+0.8)/3] = [0.5, 0.6]$
  * Properties:
    * ✅ Permutation-invariant (order doesn’t matter)
    * ✅ Simple and fast
    * ❌ Less expressive (loses structural details)
      
* **Pooling Aggregator (Best Performance)**
  * Each neighbor is transformed by an MLP.
  * Apply element-wise max-pooling (or mean-pooling).
  * Example:
    * MLP transforms neighbors (e.g., $MLP(x) = [x₁+x₂, x₁*x₂]$).
      * [0.3,0.4] → [0.7, 0.12]
      * [0.5,0.6] → [1.1, 0.30]
      * [0.7,0.8] → [1.5, 0.56]
    * Max-pooling:
      * max($[0.7,0.12], [1.1,0.30], [1.5,0.56]) = [1.5, 0.56]$
    * Properties:
      * ✅ Permutation-invariant
      * ✅ Balances expressiveness and efficiency
      * ✅ Best empirical results (accuracy + runtime)
      * Max-pooling highlights dominant neighborhood features.
        
  * **LSTM Aggregator**
    * Processes neighbors sequentially using LSTM
    * Random shuffling is applied to approximate permutation invariance
    * example: Shuffled neighbors (e.g., $u₃ → u₁ → u₂$):
    * LSTM steps:
      * Process $[0.7, 0.8]$ → hidden state $[0.6, 0.7]$
      * Process $[0.3, 0.4]$ → hidden state $[0.8, 0.9]$
      * Process $[0.5, 0.6]$ → final state $[1.0, 1.1]$
    * Properties:
      *  ❌ Not permutation-invariant (order affects output)
      *  ✅ High expressiveness (captures sequential patterns)
      *  ❌ Computationally expensive
        
  **Recommendation:**
  * Use Pooling Aggregator by default (best trade-off).
  * Use Mean for very large graphs where speed is critical.
  * Use LSTM only if neighbor order matters (rare).
    
  <img width="700" height="227" alt="image" src="https://github.com/user-attachments/assets/a4c0175c-85ca-4058-8166-e40ee828063a" />

