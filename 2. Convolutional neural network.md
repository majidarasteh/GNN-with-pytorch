# From Graph Neural Network (GNN) to Convulotional Neural Network (CNN)
## Key Drawbacks of Graph Neural Networks (GNNs) for Graph Encoding
Neural networks (NNs) are a powerful tool for processing data, but traditional fully connected NNs fail to scale efficiently for graph-structured data. Traditional Graph Neural Networks (GNNs) face several fundamental challenges that have motivated the adoption of **Convolutional Neural Networks (CNNs)** for specific graph-related tasks:

1. **Computational Inefficiency** Unlike CNNs (which share kernels across spatial locations), GNNs often process each node/edge independently, leading to high memory and compute costs for large graphs.
2. **Lack of Semantic Information** Traditional GNN architectures often treat node features as independent values without considering their higher-level semantic relationships or meanings.
3. **Non-Inductive Nature** Many GNNs operate transductively, meaning they cannot generate embeddings for unseen nodes or graphs without retraining, limiting their generalization capabilities.

## Start to use CNN
Convolutional Neural Networks (CNNs) excel on grid-structured data (e.g., images) due to translation invariance and local receptive fields. However, generalizing convolution to irregular graph structures introduces key challenges:
1. **Lack of Consistent Neighborhood Structure** Unlike grids (where each pixel has a fixed number of neighbors), graphs have variable node degrees and arbitrary connectivity.
2. **Non-Uniform Node Distances**: In grids, "distance" is well-defined (e.g., Manhattan distance). In graphs, the path length between nodes may vary (e.g., two nodes could be 1 hop or 10 hops apart).
3. **Variable Feature Dimensions**: Nodes/edges may have heterogeneous feature sizes (e.g., a molecular graph with atoms of different attribute sets).
4. **Heterogeneous Graphs**: Nodes/edges may represent different entity types (e.g., in a knowledge graph: "User," "Movie," "Genre").

## Understanding Graphs as Generic Signal Structures
1. **Traditional 1D Signals (Regular Structure)**
    Consider a 1D discrete signal, like a time series where:
     - Dimension: Time (e.g., seconds from 0 to 60).
      ![image](https://github.com/user-attachments/assets/d486b695-273c-4d9f-91be-c2024ac0125c)

     - Samples: Signal values at uniform intervals (e.g., temperature every second).
       
       ![image84](https://github.com/user-attachments/assets/5d001529-2699-41fc-abf4-4baa60c6acb2)

   **Key Property:**
     -  The spacing between samples is fixed (e.g., 1 second).
     -  Mathematically, this is a regular grid (like a line of evenly spaced points
       
   **Example:**
       ![image](https://github.com/user-attachments/assets/2ae7a945-9387-4be0-9fb0-9006f2a38c2c)
       
2.  **Irregular Signals as Graphs**
   Now, imagine the signal samples are not uniformly spaced:
    - The "distance" between samples varies
    - To model this, we need two components:
      *  Nodes: Represent signal values (like the samples).
      *  Edges: Represent the relationships or distances between samples.
   **Example:**
    - Now, suppose readings are taken at t = [0, 1.3, 2.1, 4.0] seconds:
      ![image](https://github.com/user-attachments/assets/d13768eb-4bcd-4cb6-bddb-d71c8548b6a9)

    - **Step 1: Define Nodes**
      Each time point becomes a node with:
       * Features: Temperature + optional metadata (e.g., sensor ID).
         
         ![image](https://github.com/user-attachments/assets/75f57626-b3e7-4ff2-9563-bd4f319cdfff)    

     - **Step 2: Define Edges**
      Edges encode temporal relationships or physical dependencies:
        * Edge weights: Time differences (Δt) 
          
    - **Step 3: Visualize the Graph**
      ![image](https://github.com/user-attachments/assets/5677cfa3-aa1d-40b7-8a93-5c77191113f6)

    - **Why Use a Graph Here?**
       * **Irregular Timing:** Captures variable gaps between samples.
       * **Flexible Relationships:** Edges can model for example of Temporal closeness
       * **Multi-Modal Data:** Nodes can include extra features (e.g., humidity, sensor type).

4.   **Why Graphs Are "Generic"**
      Graphs generalize signals by:
       - **Irregular Spacing:** Edges encode variable distances (unlike fixed grids).
       - **Multi-Dimensional Features:** Each node can have:
         * **Attributes:** E.g., temperature + humidity at time t.
         * **Relationships:** Edges can capture complex dependencies (e.g., causality).
     
       **Example:**
       - In a traffic network, nodes are sensors (with location + traffic data), and edges are road segments (weighted by distance/congestion).

         
5.   **Key Differences from Classical Signals**
   
   ![image](https://github.com/user-attachments/assets/ec586991-7f59-4f41-b774-93d8330921df)


**Graphs unify signal processing by treating samples as nodes and relationships as edges, enabling analysis:**
  - of Irregularly sampled data.
  - Multi-modal features (e.g., time + location + sensor type).
  - Complex dependencies (e.g., traffic flow between cities).  

## Extending Convolutional Neural Networks (CNNs) to Non-Euclidean Graphs
Convolutional Neural Networks (CNNs) excel in Euclidean spaces (e.g., images, audio) due to their grid structure and translation invariance. However, graphs are non-Euclidean—they lack:
 - Fixed node ordering (permutation invariance).
 - Uniform local neighborhoods (variable node degrees).
 - Global metrics (e.g., distance, angle).

   
**Challenges in Generalizing Convolution to Graphs**

In Euclidean spaces (e.g., images), convolution is a sliding window operation over a grid. For graphs:
 - No grid structure: Nodes are connected arbitrarily.
 - No fixed neighborhood size: Each node may have a different number of neighbors.

**Example**
 - In an image, a 3×3 kernel slides predictably.
 - In a social graph, User A might have 3 friends, while User B has 500—no fixed "kernel size" works.
   
## Extending Time-Shift Convolution in digital signal processing (DSP) to Graphs: A Formal Treatment
1. **Time-Shift Convolution in Classical DSP**

   For a 1D circular time-series signal x = [A, B, C, D]ᵀ, a time-shift is represented by matrix multiplication with a shift operator S:

   ![image](https://github.com/user-attachments/assets/4bec63ab-488e-4540-816e-3318a0c0e75a)

   ![image](https://github.com/user-attachments/assets/f72c57fa-6b24-4d75-84b9-c09420568d39)  


   Shifted signal is x' and is as follow:
   
   ![image](https://github.com/user-attachments/assets/7f2e0b21-f9a8-4f2c-aebe-64f0a6a187ee)  

   ![image](https://github.com/user-attachments/assets/14e73ba6-4a5f-4568-abe6-d67f0969454d)    
 
   Similarly, X'' effectively performing two consecutive shifts on the signal X:  
   ![image](https://github.com/user-attachments/assets/b72692a8-48c8-4069-9048-7d7a49d1af60)   


   **Key Insight:**
    - A is a **circulant matrix (adjacency matrix** of a ring graph).
    - Each multiplication A performs a circular shift.
    - Each multiplication by A shifts the signal one step forward in the circular graph.
    
2.  **Generalizing Shift to Arbitrary Graphs**

     For graphs, the "shift" operation must account for:
      - Irregular neighborhoods (no fixed shift stride).
      - Variable connectivity (non-ring topologies).

    **Solution:** Replace S with the graph adjacency matrix (A) or normalized variants.

    **Example:** 4-Node Non-Ring Graph

    Let the graph have edges:  
    A → B, B → C, C → D, D → A (ring) and A → C (extra edge).    
    ![image](https://github.com/user-attachments/assets/f16e16b8-2289-4996-bdee-31663f322ed4)
    
    **Adjacency matrix A:**  
   ![image](https://github.com/user-attachments/assets/feaca269-d288-4e12-8188-d9d662bd9d36)  

     **Sifted dignal x'=A.x**   
    ![image](https://github.com/user-attachments/assets/1434aded-812f-46e1-bf6a-442c776b9d99)

    **Interpretation:**
     - Each node aggregates values from its neighbors.
     - Unlike the ring graph, node A now sums B and C (due to the extra edge A → C).  

3. **Convolution in graphs**   
In classical DSP, applying a shift twice is equivalent to a double time delay. For graphs, this generalizes to multi-hop aggregation. For graphs, the "shift" is defined by the adjacency matrix A, and the "weighting" is learned. Hence, the grapg convolution is defined as follow (where g is a learnable weight and k is the iteration):
    
  ![image](https://github.com/user-attachments/assets/cab7fa26-c40a-4aa2-99d9-2dea9de4376b)   


4. **Practical Example**
   - **Task:** Predict the next value in the time-series using a 2-tap filter.
   - **Graph Signal:** X=[1, 2, 3, 4] and (A=1, B=2, C=3, D=4).
   - **Kernel:** g=[0.5, 0.3] (eights for current and 1-hop neighbor).
   
   **Convolution:**  
     ![image](https://github.com/user-attachments/assets/aa196087-7a26-4a2a-82d0-9cba1e34289f)   

    **Interpretation:**  
     - Node A’s output = 0.5*A + 0.3*B = 0.5*1 + 0.3*2 = 1.1.
     - Node D’s output = 0.5*D + 0.3*A = 0.5*4 + 0.3*1 = 2.3.  

   **Graph convolution generalizes DSP convolution:**  
    - Ring graph → Classical time-shift convolution.
    - Ring graph → Classical time-shift convolution.

5. **Practical Applications**
   - **Time-Series Forecasting:**  
     * $A⋅x$ predicts the next time step.
     * $A⋅x$ predicts the next time step.
   - **Graph Neural Networks (GNNs):**  
     * $A^2.x$ captures longer-range dependencies (e.g., friends-of-friends in social networks).
   - **Diffusion Processes:**
     * $A^k.x$ models how information spreads over k hops. $A^k.x$ generalizes the concept of time delays to k-hop neighborhood aggregation in graphs.
       
6. **Layer-wise Operation in Graph Convolutional Networks (GCNs)**

   Each layer of a GCN consists of:
    - Conditional Filter Banks (learnable weight matrices)
    - Graph Convolution (neighborhood aggregation via $G$)
    - Nonlinear Activation (e.g., sigmoid)

 For layer *l* in a GCN, the forward pass is computed as:  
 ![image](https://github.com/user-attachments/assets/29462741-49af-48ea-9758-3e19b890e1aa)  

 Where:  
  - $X^l$: Node feature matrix at layer *l* (N nodes, d features). Deep GCNs may lose node distinguishability (over-smoothing)). Typical depth: 2-4 layers for most applications. At first, each layer aggregates information from 1-hop neighbors. Similarly, Stacking $L$ layers captures L-hop neighborhoods
  - $G$: Graph shift operator (typically normalized adjacency matrix)
  - $W^l$: Learnable weight matrix (filter bank). $W^l$ shared across all nodes
  - $b^l$: Learnable bias term
  - $σ$: Nonlinear activation function (sigmoid in your case). For example, Sigmoid introduces nonlinear decision boundaries.

 **Layer-wise Propagation Explained**  
  - $GX^(l)$: aggregates features from each node's neighbors.
  - Feature Transformation $(XW + b)$: Projects features into new space.
  - Nonlinearity ($σ$): Bounds outputs to (0,1) for probabilistic interpretations.  
    ![image](https://github.com/user-attachments/assets/98be7803-1414-4470-a74d-4ba904f7f15a)

    For $n$ layer GCN:  
    ![image](https://github.com/user-attachments/assets/adbe8522-053f-46ee-8360-e0d9994634bc)

    ![image](https://github.com/user-attachments/assets/8f4a1755-e9ef-48ad-8aa7-92675656a9f7)

    
8. df

