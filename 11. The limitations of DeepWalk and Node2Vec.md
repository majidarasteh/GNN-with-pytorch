# The limitations of DeepWalk and Node2Vec

The **limitations of DeepWalk and Node2Vec**—computational inefficiency, lack of feature utilization, and non-inductive nature—motivate the need for more advanced techniques like graph **convolutional networks (GCNs)** such as **GraphSAGE** and **GAT** (Graph Attention Network) or other neural message-passing approaches such as **Message Passing Neural Network (MPNN)**. The limitations of DeepWalk and Node2Vec are as follow:  

1. **Parameter Sharing & Computational Efficiency:** DeepWalk/Node2Vec rely solely on graph structure (edges), ignoring node attributes (e.g., text, images, or metadata). GCNs naturally integrate node features by propagating them through the graph (e.g., via feature aggregation from neighbors). For example in a citation network, a GCN can use both citation links and paper text to generate embeddings.
   
2. **Incorporating Node Features (Semantic Information):** DeepWalk/Node2Vec rely solely on graph structure (edges), ignoring node attributes (e.g., text, images, or metadata). GCNs naturally integrate node features by propagating them through the graph (e.g., via feature aggregation from neighbors). For example in a citation network, a GCN can use both citation links and paper text to generate embeddings.
   
3. **Inductive Learning (Handling Unseen Nodes:)** DeepWalk/Node2Vec are transductive—they require retraining for new nodes. GCNs are inductive—they learn a generalizable function (e.g., a neural network) that can embed unseen nodes if their features and local structure are provided. For example a GCN trained on a social network can embed a new user without retraining, as long as their friends and profile features are known.

## Challenges in applying convolutional methods to graph-structured data  
The critical challenges in applying convolutional methods to graph-structured data (unlike grid-like images) are as follow: 
1. **Variable Number of Nodes:** Unlike images (fixed grid size), graphs can have arbitrary numbers of nodes.
2. **Variable Node Distances (No Fixed Spatial Locality):** In images, convolution relies on fixed-distance neighbors (e.g., 3x3 kernels). Graphs have irregular connectivity.
3. **Variable Number of Features per Node:** Nodes may have different feature dimensions (e.g., users with varying metadata).
4. **Heterogeneous Graphs (Nodes/Edges of Different Types):** Nodes/edges may have different types (e.g., users, products, reviews).

   ## Simple Graph Convolution
   Simple Graph Convolution (SGC) is a minimal form of graph convolution where:
   
    1. **Aggregation:** Each node collects features from its neighbors (e.g., by averaging them). For a node $v$, aggregate features from its neighbors $N(v)$:

       ![image](https://github.com/user-attachments/assets/1dc827e1-c2ef-4404-aff6-ab82f318d77a)
    
          $h_u$ : Feature vector of neighbor $u$.

          **Example:** If node $B$ has neighbors $A$ and $C$, its aggregated feature is:

       ![image](https://github.com/user-attachments/assets/308197bc-b323-4a82-a9d9-f720245ea666)

      
    2. **Update:** Replace the node’s feature with the aggregated value:

       ![image](https://github.com/user-attachments/assets/0eaaa665-07a8-4eea-b698-690680990354)

        * **No learnable parameters:** Just neighbor averaging.  
        * **With parameters:** Add a weight matrix $W$ and activation (e.g., ReLU):

        ![image](https://github.com/user-attachments/assets/ba1037d3-1df7-4542-b134-76a868daf4ac)

    **Why Is This Simple?**
   
     1. **No feature transformation:** Raw neighbor features are averaged.
     2. **No self-loops:** Node’s own features are ignored unless explicitly added.
     3. **Single-hop:** Only immediate neighbors are considered (no multi-layer propagation).
  
## Example

![image](https://github.com/user-attachments/assets/4bc5c2ef-fce4-4bfd-b199-f33f6b8a0ee1)

![image](https://github.com/user-attachments/assets/31c270c6-667c-48f7-8296-dbc882f41ce5)  

## Similarity in Embedding Space  

The goal is to measure how "similar" two nodes are based on their embeddings $V(K)$. Common similarity metrics:  

 1) **Cosine Similarity:**  Values in $[−1,1]$. Used for node similarity (e.g., recommendation systems).

     ![image](https://github.com/user-attachments/assets/65cb93d5-aa0c-4514-92de-5cc2f91ff216)

 3) **Euclidean Distance:**  Smaller distance → higher similarity.

    ![image](https://github.com/user-attachments/assets/2dbb1102-a133-4e95-8fa9-6824828f10de)

## Drawbacks of Simple Graph Convolution (SGC)  
Simple Graph Convolution (SGC) is a foundational method for graph-structured data, but it has several key limitations that hinder its performance in complex scenarios. Here are the main drawbacks:  

1) **Equal Treatment of Neighbors (No Attention Mechanism):** SGC averages all neighboring nodes equally, ignoring differences in their importance. For example in a social network, a celebrity’s post should influence a node more than a random user’s post, but SGC treats them the same. For handling this problem, Graph Attention Networks (GAT) assign learnable weights to neighbors.
2) **Fixed Averaging Aggregation (May Not Be Optimal):**  Mean aggregation assumes all neighbors contribute equally, which isn’t always true. For example In fraud detection, some transactions (edges) are more suspicious than others. For instance, GraphSAGE uses max-pooling or LSTM aggregation.
3) **SNo Node Feature Transformation (Limited Expressiveness):** SGC simply averages raw features without learning feature transformations. For example if two neighbors have opposite features (e.g., $[1,0]$ and $[-1,0]$), their average ([0,0]) loses information. For example for handling this problem GCN adds a learnable weight matrix:  

     ![image](https://github.com/user-attachments/assets/c928ee25-43f5-4227-af5a-ee474f39cd6b)  

5) **Over-Smoothing (Deep SGC Blurs Node Features):** After many layers, node embeddings become indistinguishable because repeated averaging diffuses features globally. For example, in a 10-layer SGC, nodes in a community may all converge to the same embedding.
