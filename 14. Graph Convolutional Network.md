# Graph Convolutional Network (GCN)  
the Graph Convolutional Network (GCN), introduced by Kipf & Welling in 2016 ("Semi-Supervised Classification with Graph Convolutional Networks"), remains one of the most influential papers in graph representation learning. It laid the foundation for modern Graph Neural Networks (GNNs) and inspired many subsequent advancements like GraphSAGE, GAT, and SGC. Here’s a concise breakdown of the key differences between Simple Graph Convolution (SGC) and Graph Convolutional Network (GCN). Key Innovations of GCN are as follow:  

1. **Spectral vs. Spatial Convolution:** Early graph convolutions relied on computationally expensive Fourier transforms in the spectral domain (e.g., Bruna et al., 2013). Kipf & Welling simplified this by proposing a first-order approximation of spectral convolutions, making it efficient and scalable.
2. **Neighborhood Aggregation:** GCN defines a layer-wise propagation rule for node embeddings:  
  ![image](https://github.com/user-attachments/assets/58b14556-a61d-4aff-be15-ef4793996b5c)

3. **Semi-Supervised Learning** GCN was originally designed for node classification with limited labeled data, leveraging graph structure to propagate labels efficiently.
 
## Advantages and Disadvantages
1. **Popularity of GCN**
   * Simplicity & Effectiveness: The single-hop aggregation rule is easy to implement and works surprisingly well for tasks like node classification. GCN outperformed traditional methods (e.g., label propagation) while being computationally tractable.
   * Foundational Impact: Inspired variants like GraphSAGE (inductive learning), GAT (attention-based aggregation), and SGC (linearized GCN). Also, introduced the concept of message-passing, now central to GNNs.
   * Broad Applicability: Used in social networks, citation graphs, biology (protein interactions), recommender systems, and more.
     
2. **Limitations of GCN**
    * Transductive : Requires the full graph during training and cannot generalize to unseen nodes/graphs (addressed later by GraphSAGE).
    * Over-Smoothing: Stacking too many layers leads to indistinguishable node embeddings due to excessive neighborhood mixing.
    * Homophily Assumption: Works best when connected nodes are similar (homophily). Struggles with heterophilic graphs.
    * Fixed Aggregation: Uses uniform weighting of neighbors (no attention mechanism like GAT).
      
3. **How Later Models Improved Upon GCN**
   
   ![image](https://github.com/user-attachments/assets/9e7d13c5-1707-426a-85ae-11045e3660e7)

Hence, we can use GCN when: the graph is static, fits in memory, and homophily is strong (linked nodes are similar). We should avoid GCN when: the graph is dynamic or large-scale (use GraphSAGE), neighbor relationships are complex (use GAT), and Heterophily exists (use H2GCN).

## Mathematical formulation
Below is a step-by-step explanation of how a Graph Convolutional Network (GCN) works, breaking down its key components and operations.  
1. **Input Graph Representation:**
   * **Graph structure:** Let the graph be defined as $G=(V,E)$, where $V$ is the set of nodes and $E$ is the set of edges.
   * **Adjacency Matrix ($A$):** $A$ binary matrix where $A_{ij}=1$ if there’s an edge between nodes, else 0.
   * **Node Features ($X$):** An $N×F$ matrix, where $N$ is the number of nodes and $F$ is the number of input features per node.
     
2. **Normalize the Adjacency Matrix** GCN uses a normalized adjacency matrix to avoid scaling issues during aggregation. Normalization ensures that features are averaged proportionally to node degrees (avoiding dominance by high-degree nodes).  
   ![image](https://github.com/user-attachments/assets/9cdc22dc-68fd-4014-a24e-e3220d407855)

3. **Define the GCN Layer** A single GCN layer performs two operations:  
   ![image](https://github.com/user-attachments/assets/31d2e68a-28a5-4a64-8029-b86fcef30f36)

4. **Stack Multiple Layers:** A typical GCN has 2 layers. Each layer aggregates features from 1-hop neighbors and deeper layers capture larger neighborhoods but risk over-smoothing.
   
   ![image](https://github.com/user-attachments/assets/d9cd5bfe-f584-47db-8b57-619670caa869)


5. **Training the GCN**
   
   ![image](https://github.com/user-attachments/assets/eca94d7d-9f85-4179-ab3d-4340412b11f2)

## Example 
Consider a tiny social network with 4 users and the following connections:  

1. **Define the Graph**

   ![image](https://github.com/user-attachments/assets/425cd20c-8136-46e5-948f-b6bf76a7c1df)

   ![image](https://github.com/user-attachments/assets/3e451f91-77af-4f84-9a47-8589d24fba55)

2. **Add Self-Loops and Normalize**

   ![image](https://github.com/user-attachments/assets/3b7cc1bf-1941-4f18-98e0-eba469c0f6e7)

   ![image](https://github.com/user-attachments/assets/a9e44e9e-b56c-47d1-b4fa-9145245c3298)


3. **First GCN Layer**
   
   ![image](https://github.com/user-attachments/assets/695b3ada-a57a-4b2a-88ec-430ed073a342)

   ![image](https://github.com/user-attachments/assets/2e4e9997-7aa0-4616-8705-af501dd9870c)

   ![image](https://github.com/user-attachments/assets/924ca5e0-1f7e-4352-8a58-720598195346)

4. **Second GCN Layer**

   ![image](https://github.com/user-attachments/assets/94ca747d-fbb5-4c4f-9daa-220441a18a5f)

   ![image](https://github.com/user-attachments/assets/60968359-bbdb-49bb-9d61-b1a7753d9f62)

5. **Interpretation** Output $Z$: Predicted probabilities for each user belonging to Class 0 or Class 1. For example, user 3 has the highest probability (0.55) for Class 1, likely due to their connections (User 1 and User 2 influenced their features). GCN leverages both node features and graph structure to make predictions.


