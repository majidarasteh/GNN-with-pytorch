# Graph Convolutional Network (GCN)  
the Graph Convolutional Network (GCN), introduced by Kipf & Welling in 2016 ("Semi-Supervised Classification with Graph Convolutional Networks"), remains one of the most influential papers in graph representation learning. It laid the foundation for modern Graph Neural Networks (GNNs) and inspired many subsequent advancements like GraphSAGE, GAT, and SGC. Key Innovations of GCN are as follow:  
1. **Spectral vs. Spatial Convolution:** Early graph convolutions relied on computationally expensive Fourier transforms in the spectral domain (e.g., Bruna et al., 2013). Kipf & Welling simplified this by proposing a first-order approximation of spectral convolutions, making it efficient and scalable.
2. **Neighborhood Aggregation:** GCN defines a layer-wise propagation rule for node embeddings:  
  ![image](https://github.com/user-attachments/assets/58b14556-a61d-4aff-be15-ef4793996b5c)

3. **Semi-Supervised Learning** GCN was originally designed for node classification with limited labeled data, leveraging graph structure to propagate labels efficiently.
 
## Advantages and Disadvantages
1. **Popularity of GCN**
   * Simplicity & Effectiveness: The single-hop aggregation rule is easy to implement and works surprisingly well for tasks like node classification. GCN outperformed traditional methods (e.g., label propagation) while being computationally tractable.
   * Foundational Impact: Inspired variants like GraphSAGE (inductive learning), GAT (attention-based aggregation), and SGC (linearized GCN). Also, introduced the concept of message-passing, now central to GNNs.
   * Broad Applicability: Used in social networks, citation graphs, biology (protein interactions), recommender systems, and more.
     
2. **Limitations of GCN**
    * Transductive : Requires the full graph during training and cannot generalize to unseen nodes/graphs (addressed later by GraphSAGE).
    * Over-Smoothing: Stacking too many layers leads to indistinguishable node embeddings due to excessive neighborhood mixing.
    * Homophily Assumption: Works best when connected nodes are similar (homophily). Struggles with heterophilic graphs.
    * Fixed Aggregation: Uses uniform weighting of neighbors (no attention mechanism like GAT).
      
3. **How Later Models Improved Upon GCN**
   
![image](https://github.com/user-attachments/assets/9e7d13c5-1707-426a-85ae-11045e3660e7)


