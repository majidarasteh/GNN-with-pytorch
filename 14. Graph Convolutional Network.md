# Graph Convolutional Network (GCN)  
the Graph Convolutional Network (GCN), introduced by Kipf & Welling in 2016 ("Semi-Supervised Classification with Graph Convolutional Networks"), remains one of the most influential papers in graph representation learning. It laid the foundation for modern Graph Neural Networks (GNNs) and inspired many subsequent advancements like GraphSAGE, GAT, and SGC. Key Innovations of GCN are as follow:  
1. **Spectral vs. Spatial Convolution:** Early graph convolutions relied on computationally expensive Fourier transforms in the spectral domain (e.g., Bruna et al., 2013). Kipf & Welling simplified this by proposing a first-order approximation of spectral convolutions, making it efficient and scalable.
2. **Neighborhood Aggregation:** GCN defines a layer-wise propagation rule for node embeddings:  
  ![image](https://github.com/user-attachments/assets/58b14556-a61d-4aff-be15-ef4793996b5c)

3. **Semi-Supervised Learning** GCN was originally designed for node classification with limited labeled data, leveraging graph structure to propagate labels efficiently.
 
## Advantages and Disadvantages
1. **Popularity of GCN**
   * Simplicity & Effectiveness: The single-hop aggregation rule is easy to implement and works surprisingly well for tasks like node classification. GCN outperformed traditional methods (e.g., label propagation) while being computationally tractable.
   * Foundational Impact: Inspired variants like GraphSAGE (inductive learning), GAT (attention-based aggregation), and SGC (linearized GCN). Also, introduced the concept of message-passing, now central to GNNs.
   * Broad Applicability: Used in social networks, citation graphs, biology (protein interactions), recommender systems, and more.
     
2. **Limitations of GCN**
    * Transductive : Requires the full graph during training and cannot generalize to unseen nodes/graphs (addressed later by GraphSAGE).
    * Over-Smoothing: Stacking too many layers leads to indistinguishable node embeddings due to excessive neighborhood mixing.
    * Homophily Assumption: Works best when connected nodes are similar (homophily). Struggles with heterophilic graphs.
    * Fixed Aggregation: Uses uniform weighting of neighbors (no attention mechanism like GAT).
      
3. **How Later Models Improved Upon GCN**
   
   ![image](https://github.com/user-attachments/assets/9e7d13c5-1707-426a-85ae-11045e3660e7)

Hence, we can use GCN when: the graph is static, fits in memory, and homophily is strong (linked nodes are similar). We should avoid GCN when: the graph is dynamic or large-scale (use GraphSAGE), neighbor relationships are complex (use GAT), and Heterophily exists (use H2GCN).

## Example
Below is a step-by-step explanation of how a Graph Convolutional Network (GCN) works, breaking down its key components and operations.  
1. **Input Graph Representation:**
   * **Graph structure:** Let the graph be defined as $G=(V,E)$, where $V$ is the set of nodes and $E$ is the set of edges.
   * **Adjacency Matrix ($A$):** $A$ binary matrix where $A_{ij}=1$ if there’s an edge between nodes, else 0.
   * **Node Features ($X$):** An $N×F$ matrix, where $N$ is the number of nodes and $F$ is the number of input features per node.
     
2. **Normalize the Adjacency Matrix** GCN uses a normalized adjacency matrix to avoid scaling issues during aggregation. Normalization ensures that features are averaged proportionally to node degrees (avoiding dominance by high-degree nodes).  
   ![image](https://github.com/user-attachments/assets/9cdc22dc-68fd-4014-a24e-e3220d407855)

4. fdg\
   
