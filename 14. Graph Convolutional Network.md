# Graph Convolutional Network (GCN)  
the Graph Convolutional Network (GCN), introduced by Kipf & Welling in 2016 ("Semi-Supervised Classification with Graph Convolutional Networks"), remains one of the most influential papers in graph representation learning. It laid the foundation for modern Graph Neural Networks (GNNs) and inspired many subsequent advancements like GraphSAGE, GAT, and SGC. Here’s a concise breakdown of the key differences between Simple Graph Convolution (SGC) and Graph Convolutional Network (GCN). Key Innovations of GCN are as follow:  

1. **Spectral vs. Spatial Convolution:** Early graph convolutions relied on computationally expensive Fourier transforms in the spectral domain (e.g., Bruna et al., 2013). Kipf & Welling simplified this by proposing a first-order approximation of spectral convolutions, making it efficient and scalable.
2. **Neighborhood Aggregation:** GCN defines a layer-wise propagation rule for node embeddings
3. **Semi-Supervised Learning** GCN was originally designed for node classification with limited labeled data, leveraging graph structure to propagate labels efficiently.
 
## Advantages and Disadvantages
1. **Popularity of GCN**
   * **Simplicity and Effectiveness**: The single-hop aggregation rule is a foundational concept in graph neural networks (GNNs), particularly in models like Graph Convolutional Networks (GCNs). It defines how a node updates its features by combining information from its immediate (1-hop) neighbors in the graph. The single-hop aggregation rule is easy to implement and works surprisingly well for tasks like node classification. GCN outperformed traditional methods (e.g., label propagation) while being computationally tractable.
   * **Foundational Impact**: Inspired variants like GraphSAGE (inductive learning), GAT (attention-based aggregation), and SGC (linearized GCN). Also, introduced the concept of message-passing, now central to GNNs. Message Passing is like neighbors sharing information to update their knowledge. For example, Each node (e.g., a person in a social network) collects information (messages) from its direct neighbors (1-hop connections). Then, it mixes these messages with its own features to update its "knowledge."
   * **Broad Applicability**: Used in social networks, citation graphs, biology (protein interactions), recommender systems, and more.
     
2. **Limitations of GCN**
    * **Transductive**: Transductive learning in Graph Convolutional Networks (GCNs) means the model is trained to make predictions only for nodes it has already seen during training—even if those nodes were unlabeled. This feature requires the full graph during training and cannot generalize to unseen nodes/graphs (addressed later by GraphSAGE). The model cannot predict for nodes added after training (e.g., a new user in a social network). For example, imagine training a GCN to classify research papers in a citation network (like Cora). GCN can predict labels for unlabeled papers in that same network but fails if you add a new paper later.
    * **Over-Smoothing**: Over-smoothing is the price of oversharing in GCNs. Stacking too many layers leads to indistinguishable node embeddings due to excessive neighborhood mixing. In other words, nodes lose their unique characteristics because they over-mix with distant neighbors. Over-Smoothing in GCN happens when a graph neural network becomes too "blurry"—like mixing all paint colors until everything turns gray.Why It Happens? Each GCN layer averages features from neighbors. Eventually, all nodes average over the entire graph, losing local details.
    * **Homophily Assumption**: Works best when connected nodes are similar (homophily). Struggles with heterophilic graphs.
    * **Fixed Aggregation**: Uses uniform weighting of neighbors (no attention mechanism like GAT).
      
3. **How Later Models Improved Upon GCN**
   
   ![image](https://github.com/user-attachments/assets/9e7d13c5-1707-426a-85ae-11045e3660e7)

Hence, we can use GCN when: the graph is static, fits in memory, and homophily is strong (linked nodes are similar). We should avoid GCN when: the graph is dynamic or large-scale (use GraphSAGE), neighbor relationships are complex (use GAT), and Heterophily exists (use H2GCN).

At first, we introduce spectral approaches to graph neural networks, which operate in the spectral domain by leveraging the eigendecomposition of the graph Laplacian to define convolutional filters. These methods, such as those proposed by Bruna et al. (2014) and Defferrard et al. (2016), provide a mathematically rigorous framework but suffer from high computational costs and lack of spatial localization. 

## Spectral Graph Convolution
Spectral graph convolution is a way to apply "filters" (like in image processing) to graph data, allowing us to extract useful patterns from nodes and their connections. Imagine your graph is a spiderweb (or a mesh of springs). Nodes are Points where the web connects and edges are prings that link these points. If you pluck one node (like flicking the web), vibrations spread through the entire network. Hence, low-frequency vibrations causes slow, smooth waves (entire sections move together). On the other hand, high-frequency vibrations causes fast, jagged movements (nearby nodes move oppositely).  

1. **Eigenvalues (Λ) = "Vibration Frequencies"** Each eigenvalue ($λ$) of the Laplacian matrix ($L$) represents a unique frequency at which the graph can vibrate. Eigenvalues ($λ$) denotes to how "fast" the graph vibrates (frequencies).
   * **$λ = 0$:** The graph’s "DC component" (no vibration, just a constant value).
   * **Small $λ > 0$:** Slow, smooth vibrations (e.g., communities moving together). For example, in a social network $λ = 0.1$ means friends share similar interests (smooth).
   * **Large $λ$:** Fast, chaotic vibrations (e.g., nodes fighting their neighbors). For example, in a social network $λ = 4.0$ means trolls arguing with everyone (chaotic).
     
2. **Eigenvectors (U) = "Vibration Patterns"** Each eigenvector ($u$) describes how nodes move at a specific frequency. Low-$λ$ eigenvector means all nodes move in the same direction (global trend) and high-$λ$ eigenvector means nearby nodes move in opposite directions (local conflict). Eigenvectors ($u$) denotes to how nodes move together/oppositely (patterns). For example in a simple 3-node graph:
   * Eigenvector for λ = 0: [1, 1, 1] (all nodes identical → no variation).
   * Eigenvector for λ = 2: [1, 0, -1] (Node 0 and Node 2 oppose each other).
  
3. **Spectral Filtering**

   ![image](https://github.com/user-attachments/assets/ea7095f6-81ee-4e6a-8161-6890df639b32)

     
4. **Example: Eigenvalues & Eigenvectors of a Simple Graph**
   
   ![image](https://github.com/user-attachments/assets/b1c66213-e811-424f-86c6-33a9fd1d7bd8)

   ![image](https://github.com/user-attachments/assets/77934190-4c53-4dae-9392-a096fb1df81c)

   ![image](https://github.com/user-attachments/assets/fb36b5ef-af87-463d-bc56-d55efe15507c)

   ![image](https://github.com/user-attachments/assets/d1414336-7822-4f27-956a-e56ab9ff8ff2)

   ![image](https://github.com/user-attachments/assets/8047469a-5f26-4d99-9ef7-3fac4e5c0add)

   ![image](https://github.com/user-attachments/assets/753dc4ce-39e6-4a1f-8a61-307d0eea8e50)

5. **Interpretation**
   * $λ=0$: Corresponds to the "DC component" (constant eigenvector).
   * $λ=1$: Captures "smooth" variation (Node 0 and Node 2 oppose Node 1).
   * $λ=3$: Represents "high-frequency" variation - Sharp conflict (Node 1 opposes Nodes 0 and 2).

To address the limitations of spectral approaches, Graph Convolutional Networks (GCNs) were introduced, offering a simplified and efficient alternative. GCNs approximate spectral filters using a first-order Chebyshev polynomial, enabling localized feature aggregation directly in the spatial domain. By normalizing the adjacency matrix and incorporating self-loops, GCNs achieve strong performance with linear computational complexity, making them scalable to large graphs. This transition from spectral methods to GCNs marks a pivotal shift toward practical and scalable graph representation learning.  

## Mathematical formulation of GCN
Below is a step-by-step explanation of how a Graph Convolutional Network (GCN) works, breaking down its key components and operations.  
1. **Input Graph Representation:**
   * **Graph structure:** Let the graph be defined as $G=(V,E)$, where $V$ is the set of nodes and $E$ is the set of edges.
   * **Adjacency Matrix ($A$):** $A$ binary matrix where $A_{ij}=1$ if there’s an edge between nodes, else 0.
   * **Node Features ($X$):** An $N×F$ matrix, where $N$ is the number of nodes and $F$ is the number of input features per node.
     
2. **Normalize the Adjacency Matrix** GCN uses a normalized adjacency matrix to avoid scaling issues during aggregation. Normalization ensures that features are averaged proportionally to node degrees (avoiding dominance by high-degree nodes).  
   ![image](https://github.com/user-attachments/assets/9cdc22dc-68fd-4014-a24e-e3220d407855)

3. **Define the GCN Layer** A single GCN layer performs two operations:  
   ![image](https://github.com/user-attachments/assets/31d2e68a-28a5-4a64-8029-b86fcef30f36)

4. **Stack Multiple Layers:** A typical GCN has 2 layers. Each layer aggregates features from 1-hop neighbors and deeper layers capture larger neighborhoods but risk over-smoothing.
   
   ![image](https://github.com/user-attachments/assets/d9cd5bfe-f584-47db-8b57-619670caa869)


5. **Training the GCN**
   
   ![image](https://github.com/user-attachments/assets/eca94d7d-9f85-4179-ab3d-4340412b11f2)

## Step by Step Example 
Consider a tiny social network with 4 users and the following connections:  

1. **Define the Graph**

   ![image](https://github.com/user-attachments/assets/425cd20c-8136-46e5-948f-b6bf76a7c1df)

   ![image](https://github.com/user-attachments/assets/3e451f91-77af-4f84-9a47-8589d24fba55)

2. **Add Self-Loops and Normalize**

   ![image](https://github.com/user-attachments/assets/3b7cc1bf-1941-4f18-98e0-eba469c0f6e7)

   ![image](https://github.com/user-attachments/assets/a9e44e9e-b56c-47d1-b4fa-9145245c3298)


3. **First GCN Layer**
   
   ![image](https://github.com/user-attachments/assets/695b3ada-a57a-4b2a-88ec-430ed073a342)

   ![image](https://github.com/user-attachments/assets/2e4e9997-7aa0-4616-8705-af501dd9870c)

   ![image](https://github.com/user-attachments/assets/924ca5e0-1f7e-4352-8a58-720598195346)

4. **Second GCN Layer**

   ![image](https://github.com/user-attachments/assets/94ca747d-fbb5-4c4f-9daa-220441a18a5f)

   ![image](https://github.com/user-attachments/assets/60968359-bbdb-49bb-9d61-b1a7753d9f62)

5. **Interpretation** Output $Z$: Predicted probabilities for each user belonging to Class 0 or Class 1. For example, user 3 has the highest probability (0.55) for Class 1, likely due to their connections (User 1 and User 2 influenced their features). GCN leverages both node features and graph structure to make predictions.

## Summary

Now that we understand eigenvalues (frequencies) and eigenvectors (vibration patterns), let’s connect this to modern Graph Neural Networks (GNNs) like GCNs. Here’s a simple map between GCN and spectral Mmethods:  

![image](https://github.com/user-attachments/assets/44bde086-d657-4c17-8b74-b21b11877e35)

**Mathematical Connection**  

![image](https://github.com/user-attachments/assets/e286cadb-e158-4e63-9753-137db4abba68)

**GCN wins spectral graph convolution in practice because avoids computational bottleneck (No Eigenvectors) and Scales to graphs with millions of nodes. GCN loses some precision but gains speed and scalability.**
