# Graph Embedding: Mapping Graph Data to Latent Representations
The core objective of embedding is to **transform data** from its original complex space (e.g., graph nodes with irregular connections) into a **simplified, structured latent space (e.g., low-dimensional vectors)** while preserving essential information. This transformation is achieved through an **encoder function**, which maps input data to its embedded representation.  

**A) Key Components**
 1. **Encoder Function:** A mathematical model (e.g., neural network, matrix factorization) that converts raw data into embeddings. **Example: For a node $v$, the encoder computes: $Z_v=Encoder(u)$
    
 2. **Preservation of Information:** The encoder must retain:
    - Structural properties (e.g., node neighborhoods, graph topology)  
    - Feature semantics (e.g., similar nodes map to nearby vectors).  
      * **Trade-off:** Simplicity (lower dimensions) vs. fidelity (minimal information loss).
        
 3. **Simplification Goals:**
     - **Dimensionality reduction:** Compress sparse graph data (e.g., adjacency matrices) into dense vectors.
     - **Computational efficiency:** Enable downstream tasks (e.g., classification, clustering) on standard ML models.

**B) For example:**  
  1. **Input:** Graph $G=(V,E)$ with node features $X$  
  2. **Encoder:** A GNN layer:   
     ![image](https://github.com/user-attachments/assets/466fa878-9593-4d26-b6ee-a646c970c630)  
  3. **Output:** Embeddings ${Z_v}v∈V$ usable for tasks like link prediction.  


**C) Why embedding**
  1. **Standardization:** Converts graphs into fixed-size inputs for ML models.
  2. **Generalization:** Captures latent patterns (e.g., social communities in a network).
  3. **Visualization:** 2D/3D projections reveal clusters or anomalies.

**D) Popular Graph Embedding Encoders: DeepWalk, Node2Vec, and GraphSAGE**  
Graph embedding methods learn low-dimensional representations of nodes, edges, or entire graphs while preserving structural and semantic relationships. Below, we compare three widely used encoder approaches:

 1. **DeepWalk** Uses basic random walks (like taking random steps in a city).
     - Need something quick? → DeepWalk
 2. **Node2Vec** Uses smarter walks (like sometimes exploring new areas, sometimes staying local).
    - Care about node positions? → Node2Vec
 3. **GraphSAGE** Uses neighbor information and works with new/unseen nodes.
     - Have node features/new nodes? → GraphSAGE   
   
 ![image](https://github.com/user-attachments/assets/7c4c9a46-0fd3-4860-8dce-7777421ffda8)

   
Embedding via an encoder strikes a balance between simplicity (reduced dimensions) and information retention (preserved relationships). The choice of encoder depends on whether the priority is structure-awareness, feature integration, or computational scalability.  
