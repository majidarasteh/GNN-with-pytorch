# Graph Embedding: Mapping Graph Data to Latent Representations
The core objective of embedding is to **transform data** from its original complex space (e.g., graph nodes with irregular connections) into a **simplified, structured latent space (e.g., low-dimensional vectors)** while preserving essential information. This transformation is achieved through an **encoder function**, which maps input data to its embedded representation.  

**Key Components**
 1. **Encoder Function:** A mathematical model (e.g., neural network, matrix factorization) that converts raw data into embeddings. **Example: For a node $v$, the encoder computes: $Z_v=Encoder(u)$
    
 2. **Preservation of Information:** The encoder must retain:
    - Structural properties (e.g., node neighborhoods, graph topology)  
    - Feature semantics (e.g., similar nodes map to nearby vectors).  
      * **Trade-off:** Simplicity (lower dimensions) vs. fidelity (minimal information loss).
        
 3. **Simplification Goals:**
     - **Dimensionality reduction:** Compress sparse graph data (e.g., adjacency matrices) into dense vectors.
     - **Computational efficiency:** Enable downstream tasks (e.g., classification, clustering) on standard ML models.

**For example:**  
  1. **Input:** Graph $G=(V,E)$ with node features $X$  
  2. **Encoder:** A GNN layer:   
     ![image](https://github.com/user-attachments/assets/466fa878-9593-4d26-b6ee-a646c970c630)  
  3. **Output:** Embeddings ${Z_v}vâˆˆV$ usable for tasks like link prediction.  


**Why embedding**
