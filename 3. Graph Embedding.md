# Graph Embedding: Mapping Graph Data to Latent Representations
The core objective of embedding is to **transform data** from its original complex space (e.g., graph nodes with irregular connections) into a **simplified, structured latent space (e.g., low-dimensional vectors)** while preserving essential information. This transformation is achieved through an **encoder function**, which maps input data to its embedded representation.  

![image](https://github.com/user-attachments/assets/0b356f90-b625-49a3-ad5a-fcf55fd1eaf7)  


**A) Key Components**
 1. **Encoder Function:** A mathematical model (e.g., neural network, matrix factorization) that converts raw data into embeddings. **Example: For a node $v$, the encoder computes: $Z_v=Encoder(v)$
    
 2. **Preservation of Information:** The encoder must retain:
    - Structural properties (e.g., node neighborhoods, graph topology)  
    - Feature semantics (e.g., similar nodes map to nearby vectors).  
      * **Trade-off:** Simplicity (lower dimensions) vs. fidelity (minimal information loss).
        
 3. **Simplification Goals:**
     - **Dimensionality reduction:** Compress sparse graph data (e.g., adjacency matrices) into dense vectors.   
       ![image](https://github.com/user-attachments/assets/6dcd49d7-33ed-4d5c-a674-845136841be1)

     - **Computational efficiency:** Enable downstream tasks (e.g., classification, clustering) on standard ML models.

**B) For example:**  
  1. **Input:** Graph $G=(V,E)$ with node features $X$  
  2. **Encoder:** A GNN layer:   
     ![image](https://github.com/user-attachments/assets/466fa878-9593-4d26-b6ee-a646c970c630)  
  3. **Output:** Embeddings ${Z_v}v∈V$ usable for tasks like link prediction.  


**C) Why embedding**
  1. **Standardization:** Converts graphs into fixed-size inputs for ML models.
  2. **Generalization:** Captures latent patterns (e.g., social communities in a network).
  3. **Visualization:** 2D/3D projections reveal clusters or anomalies.

**D) Core objective**  

The core objective of graph embedding is to preserve meaningful relationships from the original graph in the learned embedding space. **Preserve similarity is the primary objective of embedding**:   
![image](https://github.com/user-attachments/assets/5a6ac4a8-3383-4d8e-b298-10a873826e7f)   

**E) Two Fundamental Questions in Graph Embedding**  

  1. **How to Perform Encoding?** The encoding process defines how we transform graph data into embeddings. Common approaches include:
      - **Message Passing (GNN-style)**
        * Aggregation: Collect information from neighbors (e.g., mean, sum, max).
        * Update: Combine aggregated message with the node’s current state. 
      - **Random Walk + Skip-gram (DeepWalk/Node2Vec):**
        * Generate random walks (uniform or biased).
        * Optimize embeddings to predict co-occurring nodes in walks (Skip-gram).
          
          ![image](https://github.com/user-attachments/assets/4150760e-1e8d-4d41-82f5-5c4827df5540)
          
  2. **What is the Meaning of Similarity?** Similarity definitions vary between the original graph and embedding space:
     * **Original Graph Similarity ($sim_G$)**
       - Structural Similarity (e.g. Adjacency: Linked nodes are similar.)
       - Feature Similarity: Nodes with close attributes (e.g., user profiles).
       - Higher-Order: Roles (e.g., hubs, bridges) or community membership.  
     * **Embedding Space Similarity ($sim_E$)**
       - Dot Product / Cosine:  
          ![image](https://github.com/user-attachments/assets/655d73e8-c86f-4109-837b-eb9327135e60)

       - Dot Product / Cosine:  
         ![image](https://github.com/user-attachments/assets/be588b6e-3064-48fe-ba4a-f130bb48162d)

     **The encoding function must ensure that**
     
     ![image](https://github.com/user-attachments/assets/23331180-2b37-46e3-a554-8307352bb3f8)

**F) Popular Graph Embedding Encoders: DeepWalk, Node2Vec, and GraphSAGE**  

Graph embedding methods learn low-dimensional representations of nodes, edges, or entire graphs while preserving structural and semantic relationships. Below, we compare three widely used encoder approaches:

 1. **DeepWalk** Uses basic random walks (like taking random steps in a city).
     - Need something quick? → DeepWalk
 2. **Node2Vec** Uses smarter walks (like sometimes exploring new areas, sometimes staying local).
    - Care about node positions? → Node2Vec
 3. **GraphSAGE** Uses neighbor information and works with new/unseen nodes.
     - Have node features/new nodes? → GraphSAGE   
   
 ![image](https://github.com/user-attachments/assets/7c4c9a46-0fd3-4860-8dce-7777421ffda8)

Embedding via an encoder strikes a balance between simplicity (reduced dimensions) and information retention (preserved relationships). The choice of encoder depends on whether the priority is structure-awareness, feature integration, or computational scalability.  
